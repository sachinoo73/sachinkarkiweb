recipe(host_is_superhost ~ ., data = ex_listing) %>%
step_string2factor(all_nominal(), -all_outcomes()) %>%
prep(data = dataset)
}
recipe_prepped <- recipe_simple(dataset = train_tbl)
train_baked <- bake(recipe_prepped, new_data = train_tbl)
test_baked  <- bake(recipe_prepped, new_data = test_tbl)
train_baked <- bake(recipe_prepped, new_data = train_tbl)
test_baked  <- bake(recipe_prepped, new_data = test_tbl)
logistic_glm <-
logistic_reg(mode = "classification") %>%
set_engine("glm") %>%
fit(host_is_superhost ~ ., data = train_baked)
logistic_glm <-
logistic_reg(mode = "classification") %>%
set_engine("glm") %>%
fit(host_is_superhost ~ ., data = train_baked)
logistic_glm <-
logistic_reg(mode = "regression") %>%
set_engine("glm") %>%
fit(host_is_superhost ~ ., data = train_baked)
logistic_glm <-
logistic_reg(mode = "classification") %>%
set_engine("glm") %>%
fit(host_is_superhost ~ ., data = train_baked)
ex_listing <- ex_listing %>%
mutate(host_is_superhost = factor(host_is_superhost))
set.seed(seed = 1972)
train_test_split <-
rsample::initial_split(
data = ex_listing,
prop = 0.80
)
train_test_split
## <5626/1406/7032>
train_tbl <- train_test_split %>% training()
test_tbl  <- train_test_split %>% testing()
recipe_prepped <- recipe_simple(dataset = train_tbl)
train_baked <- bake(recipe_prepped, new_data = train_tbl)
test_baked  <- bake(recipe_prepped, new_data = test_tbl)
logistic_glm <-
logistic_reg(mode = "classification") %>%
set_engine("glm") %>%
fit(host_is_superhost ~ ., data = train_baked)
logistic_glm <-
logistic_reg(mode = "classification") %>%
set_engine("glmnet") %>%
fit(host_is_superhost ~ ., data = train_baked)
logistic_glm <-
logistic_reg(mode = "classification") %>%
set_engine("stan") %>%
fit(host_is_superhost ~ ., data = train_baked)
logistic_glm <-
logistic_reg(mode = "classification") %>%
set_engine("keras") %>%
fit(host_is_superhost ~ ., data = train_baked)
installed.packages('keras','stan')
logistic_glm <-
logistic_reg(mode = "classification") %>%
set_engine("keras") %>%
fit(host_is_superhost ~ ., data = train_baked)
logistic_glm <-
logistic_reg(mode = "classification") %>%
set_engine("glm") %>%
fit(host_is_superhost ~ ., data = train_baked)
knitr::opts_chunk$set(echo = TRUE)
packages = c('tidyverse', 'tidytext', 'dplyr','wordcloud','wordcloud2','reshape2','topicmodels','tm','ggplot2','ggstatsplot')
for(p in packages){
if(!require(p, character.only = T)){
install.packages(p)
}
library(p, character.only = T)
}
reviews <- read_csv("data/clean/reviews.csv")
alreviews <- reviews %>%
mutate(comments = gsub(x = comments, pattern = "[^[:alpha:]]", " "))
tidyreviews <- alreviews %>%
group_by(listing_id) %>%
unnest_tokens(word,comments) %>%
group_by(word) %>%
filter(n()>5) %>%
ungroup()
tidytext::stop_words
cleanreviews <- tidyreviews %>%
anti_join(stop_words)
reviewsentiment <- cleanreviews %>%
filter() %>%
group_by(word) %>%
dplyr::summarise(frequency=n())
reviewsentiment %>% wordcloud2()
summary(review_affin_lexicon)
review_affin_lexicon <- reviewsentiment %>%
dplyr::select(word) %>%
inner_join(get_sentiments("afinn")) %>%
mutate(sentiment=ifelse(value>0,"positive","negative"),score=value)
review_affin_lexicon
ggplot(review_affin_lexicon,aes(x=reorder(word,score),y=score,colour=sentiment,fill=sentiment))+
geom_col(alpha=0.5)+
coord_flip()
review_bing_lexicon <- reviewsentiment %>%
inner_join(get_sentiments("bing")) %>%
dplyr::count(word,sentiment,frequency,sort=TRUE) %>%
acast(word~sentiment,value.var="frequency",fill=0) %>%
comparison.cloud(max.words = 150)
#Term frequencies across essays
tidy_review <- select_(reviews,"comments")
n <- nrow(tidy_review)
tidy_review <- data_frame(review=1:n, text=reviews$comments)
# Tokenize, remove stop words and NAs
data("stop_words")
#  Add some custom stop words, just to illstrate
myterms <- c("airbnb","very", "boston","stay")
mylex <- rep("custom", length(myterms))
myStopwords <- data.frame(word=myterms, lexicon=mylex)
myStopwords <- rbind(stop_words, myStopwords)
#Term frequencies across essays
review_words <- tidy_review %>%
unnest_tokens(word, text) %>%
anti_join(myStopwords) %>%
count(review, word, sort=TRUE) %>%
ungroup()
total_words <- review_words %>%
group_by(review) %>%
summarize(total = sum(n))
review_words <- left_join(review_words, total_words)
#  now tf_idf
review_words <- review_words %>%
bind_tf_idf(word, review, n) %>%
arrange(desc(tf_idf))
glimpse(review_words)
our_dtm <- review_words %>%
cast_dtm(review, word, n)
our_dtm
our_lda <- LDA(our_dtm, k = 4, control = list(seed = 10000))
our_lda
rev_topics <- tidy(our_lda, matrix = "beta")
rev_top_terms <- rev_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
rev_top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
listings <- read_csv("data/clean/listing_en.csv")
verified_listings <- listings %>% drop_na(host_identity_verified)
ggplot(verified_listings, aes(x = host_identity_verified, y = review_scores_rating)) +
geom_point(position=position_jitter(width=0.2, height=0.1)) +
geom_boxplot(outlier.colour=NA, fill=NA, colour="grey20")
ggbetweenstats(
data = verified_listings,
x = host_identity_verified,
y = review_scores_rating
)
listings1 <- listings %>% group_by(neighbourhood_cleansed, neighbourhood_group_cleansed) %>%
summarise(n = n()) %>%
arrange(desc(n)) %>%
ungroup() %>%
mutate(rank = rank(desc(n))) %>%
filter(rank <31)
ggplot(listings1, aes(x = reorder(neighbourhood_cleansed, n), y = n, fill = neighbourhood_group_cleansed)) +
geom_col() +
coord_flip()
listing2 <- listings %>% mutate(price = gsub(x = price, pattern = "[$]", ""))
listing2$price <- as.numeric(as.character(listing2$price))
ggplot(listing2, aes(x = price)) +
geom_histogram(fill = 'light blue')
training <- listing2 %>% mutate(record = 1:n()) %>%
filter(record %>% 5 != 0)
knitr::opts_chunk$set(echo = TRUE)
packages = c('tidyverse', 'tidytext', 'dplyr','wordcloud','wordcloud2','reshape2','topicmodels','tm','ggplot2','ggstatsplot')
for(p in packages){
if(!require(p, character.only = T)){
install.packages(p)
}
library(p, character.only = T)
}
reviews <- read_csv("data/clean/reviews.csv")
alreviews <- reviews %>%
mutate(comments = gsub(x = comments, pattern = "[^[:alpha:]]", " "))
tidyreviews <- alreviews %>%
group_by(listing_id) %>%
unnest_tokens(word,comments) %>%
group_by(word) %>%
filter(n()>5) %>%
ungroup()
tidytext::stop_words
cleanreviews <- tidyreviews %>%
anti_join(stop_words)
reviewsentiment <- cleanreviews %>%
filter() %>%
group_by(word) %>%
dplyr::summarise(frequency=n())
reviewsentiment %>% wordcloud2()
summary(review_affin_lexicon)
review_affin_lexicon <- reviewsentiment %>%
dplyr::select(word) %>%
inner_join(get_sentiments("afinn")) %>%
mutate(sentiment=ifelse(value>0,"positive","negative"),score=value)
review_affin_lexicon
ggplot(review_affin_lexicon,aes(x=reorder(word,score),y=score,colour=sentiment,fill=sentiment))+
geom_col(alpha=0.5)+
coord_flip()
review_bing_lexicon <- reviewsentiment %>%
inner_join(get_sentiments("bing")) %>%
dplyr::count(word,sentiment,frequency,sort=TRUE) %>%
acast(word~sentiment,value.var="frequency",fill=0) %>%
comparison.cloud(max.words = 150)
#Term frequencies across essays
tidy_review <- select_(reviews,"comments")
n <- nrow(tidy_review)
tidy_review <- data_frame(review=1:n, text=reviews$comments)
# Tokenize, remove stop words and NAs
data("stop_words")
#  Add some custom stop words, just to illstrate
myterms <- c("airbnb","very", "boston","stay")
mylex <- rep("custom", length(myterms))
myStopwords <- data.frame(word=myterms, lexicon=mylex)
myStopwords <- rbind(stop_words, myStopwords)
#Term frequencies across essays
review_words <- tidy_review %>%
unnest_tokens(word, text) %>%
anti_join(myStopwords) %>%
count(review, word, sort=TRUE) %>%
ungroup()
total_words <- review_words %>%
group_by(review) %>%
summarize(total = sum(n))
review_words <- left_join(review_words, total_words)
#  now tf_idf
review_words <- review_words %>%
bind_tf_idf(word, review, n) %>%
arrange(desc(tf_idf))
glimpse(review_words)
our_dtm <- review_words %>%
cast_dtm(review, word, n)
our_dtm
our_lda <- LDA(our_dtm, k = 4, control = list(seed = 10000))
our_lda
rev_topics <- tidy(our_lda, matrix = "beta")
rev_top_terms <- rev_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
rev_top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
listings <- read_csv("data/clean/listing_en.csv")
verified_listings <- listings %>% drop_na(host_identity_verified)
ggplot(verified_listings, aes(x = host_identity_verified, y = review_scores_rating)) +
geom_point(position=position_jitter(width=0.2, height=0.1)) +
geom_boxplot(outlier.colour=NA, fill=NA, colour="grey20")
ggbetweenstats(
data = verified_listings,
x = host_identity_verified,
y = review_scores_rating
)
listings1 <- listings %>% group_by(neighbourhood_cleansed, neighbourhood_group_cleansed) %>%
summarise(n = n()) %>%
arrange(desc(n)) %>%
ungroup() %>%
mutate(rank = rank(desc(n))) %>%
filter(rank <31)
ggplot(listings1, aes(x = reorder(neighbourhood_cleansed, n), y = n, fill = neighbourhood_group_cleansed)) +
geom_col() +
coord_flip()
listing2 <- listings %>% mutate(price = gsub(x = price, pattern = "[$]", ""))
listing2$price <- as.numeric(as.character(listing2$price))
ggplot(listing2, aes(x = price)) +
geom_histogram(fill = 'light blue')
knitr::opts_chunk$set(echo = TRUE)
#Term frequencies across essays
tidy_review <- select_(alreviews,"comments")
knitr::opts_chunk$set(echo = TRUE)
library(tidytext)
library(dplyr)
library(tidyverse)
library(wordcloud)
library(wordcloud2)
library(reshape2)
library(topicmodels)
library(ggplot2)
library(ggstatsplot)
library(tidymodels)
library(funModeling)
library(skimr)
library(DataExplorer)
library(haven)
library(plotly)
reviews <- read_csv("data/clean/reviews.csv")
listing <- read_csv("data/clean/listing_en.csv")
alreviews <- reviews %>%
mutate(comments = gsub(x = comments, pattern = "[^[:alpha:]]", " "))
tidyreviews <- alreviews %>%
group_by(listing_id) %>%
unnest_tokens(word,comments) %>%
group_by(word) %>%
filter(n()>5) %>%
ungroup()
tidytext::stop_words
cleanreviews <- tidyreviews %>%
anti_join(stop_words)
reviewsentiment <- cleanreviews %>%
filter() %>%
group_by(word) %>%
dplyr::summarise(frequency=n())
reviewsentiment %>% wordcloud2()
review_affin_lexicon <- reviewsentiment %>%
dplyr::select(word) %>%
inner_join(get_sentiments("afinn")) %>%
mutate(sentiment=ifelse(value>0,"positive","negative"),score=value)
review_affin_lexicon
ggplot(review_affin_lexicon,aes(x=reorder(word,score),y=score,colour=sentiment,fill=sentiment))+
geom_col(alpha=0.5)+
coord_flip()
review_bing_lexicon <- reviewsentiment %>%
inner_join(get_sentiments("bing")) %>%
dplyr::count(word,sentiment,frequency,sort=TRUE) %>%
acast(word~sentiment,value.var="frequency",fill=0) %>%
comparison.cloud(max.words = 150)
#Term frequencies across essays
tidy_review <- select_(alreviews,"comments")
n <- nrow(tidy_review)
tidy_review <- data_frame(review=1:n, text=reviews$comments)
# Tokenize, remove stop words and NAs
data("stop_words")
#  Add some custom stop words, just to illstrate
myterms <- c("airbnb","very","stay", "singapore", "host") #can build model to add in custom stopwords?
mylex <- rep("custom", length(myterms))
myStopwords <- data.frame(word=myterms, lexicon=mylex)
myStopwords <- rbind(stop_words, myStopwords)
#Term frequencies across essays
review_words <- tidy_review %>%
unnest_tokens(word, text) %>%
anti_join(myStopwords) %>%
count(review, word, sort=TRUE) %>%
ungroup()
total_words <- review_words %>%
group_by(review) %>%
summarize(total = sum(n))
#Term frequencies across essays
tidy_review <- select_(alreviews,"comments")
n <- nrow(tidy_review)
tidy_review <- data_frame(review=1:n, text=reviews$comments)
# Tokenize, remove stop words and NAs
data("stop_words")
#  Add some custom stop words, just to illstrate
myterms <- c("airbnb","very","stay", "singapore", "host") #can build model to add in custom stopwords?
mylex <- rep("custom", length(myterms))
myStopwords <- data.frame(word=myterms, lexicon=mylex)
myStopwords <- rbind(stop_words, myStopwords)
#Term frequencies across essays
review_words <- tidy_review %>%
unnest_tokens(word, text) %>%
anti_join(myStopwords) %>%
count(review, word, sort=TRUE) %>%
ungroup()
review_words <- left_join(review_words, total_words)
#  now tf_idf
review_words <- review_words %>%
bind_tf_idf(word, review, n) %>%
arrange(desc(tf_idf))
glimpse(review_words)
#Term frequencies
tidy_review <- select_(alreviews,"comments")
n <- nrow(tidy_review)
tidy_review <- data_frame(review=1:n, text=reviews$comments)
# Tokenize, remove stop words and NAs
data("stop_words")
#  Add some custom stop words
myterms <- c("airbnb","very","stay", "singapore", "host") #can build model to add in custom stopwords?
mylex <- rep("custom", length(myterms))
myStopwords <- data.frame(word=myterms, lexicon=mylex)
myStopwords <- rbind(stop_words, myStopwords)
#Term frequencies
review_words <- tidy_review %>%
unnest_tokens(word, text) %>%
anti_join(myStopwords) %>%
count(review, word, sort=TRUE) %>%
ungroup()
review_words <- left_join(review_words, total_words)
#  now tf_idf
review_words <- review_words %>%
bind_tf_idf(word, review, n) %>%
arrange(desc(tf_idf))
glimpse(review_words)
#Term frequencies
tidy_review <- select_(alreviews,"comments")
n <- nrow(tidy_review)
tidy_review <- data_frame(review=1:n, text=reviews$comments)
# Tokenize, remove stop words and NAs
data("stop_words")
#  Add some custom stop words
myterms <- c("airbnb","very","stay", "singapore", "host") #can build model to add in custom stopwords?
mylex <- rep("custom", length(myterms))
myStopwords <- data.frame(word=myterms, lexicon=mylex)
myStopwords <- rbind(stop_words, myStopwords)
#Term frequencies
review_words <- tidy_review %>%
unnest_tokens(word, text) %>%
anti_join(myStopwords) %>%
count(review, word, sort=TRUE) %>%
ungroup()
review_words <- left_join(review_words, total_words)
#  now tf_idf
review_words <- review_words %>%
bind_tf_idf(word, review, n) %>%
arrange(desc(tf_idf))
glimpse(review_words)
#Term frequencies across essays
tidy_review <- select_(reviews,"comments")
n <- nrow(tidy_review)
tidy_review <- data_frame(review=1:n, text=reviews$comments)
# Tokenize, remove stop words and NAs
data("stop_words")
#  Add some custom stop words, just to illstrate
myterms <- c("airbnb","very", "boston","stay")
mylex <- rep("custom", length(myterms))
myStopwords <- data.frame(word=myterms, lexicon=mylex)
myStopwords <- rbind(stop_words, myStopwords)
#Term frequencies across essays
review_words <- tidy_review %>%
unnest_tokens(word, text) %>%
anti_join(myStopwords) %>%
count(review, word, sort=TRUE) %>%
ungroup()
total_words <- review_words %>%
group_by(review) %>%
summarize(total = sum(n))
#Term frequencies across essays
tidy_review <- select_(reviews,"comments")
n <- nrow(tidy_review)
tidy_review <- data_frame(review=1:n, text=reviews$comments)
# Tokenize, remove stop words and NAs
data("stop_words")
#  Add some custom stop words
myterms <- c("airbnb","very", "singapore","stay")
mylex <- rep("custom", length(myterms))
myStopwords <- data.frame(word=myterms, lexicon=mylex)
myStopwords <- rbind(stop_words, myStopwords)
#Term frequencies
review_words <- tidy_review %>%
unnest_tokens(word, text) %>%
anti_join(myStopwords) %>%
count(review, word, sort=TRUE) %>%
ungroup()
total_words <- review_words %>%
group_by(review) %>%
summarize(total = sum(n))
str(review_words)
total_words <- review_words %>%
group_by(as.numeric(review)) %>%
summarize(total = sum(n))
#Term frequencies across essays
tidy_review <- select_(reviews,"comments")
n <- nrow(tidy_review)
tidy_review <- data_frame(review=1:n, text=reviews$comments)
# Tokenize, remove stop words and NAs
data("stop_words")
#  Add some custom stop words
myterms <- c("airbnb","very", "singapore","stay")
mylex <- rep("custom", length(myterms))
myStopwords <- data.frame(word=myterms, lexicon=mylex)
myStopwords <- rbind(stop_words, myStopwords)
#Term frequencies
review_words <- tidy_review %>%
unnest_tokens(word, text) %>%
anti_join(myStopwords) %>%
count(review, word, sort=TRUE) %>%
ungroup()
total_words <- review_words %>%
group_by(review) %>%
summarise(total = sum(n))
review_words <- left_join(review_words, total_words)
#  now tf_idf
review_words <- review_words %>%
bind_tf_idf(word, review, n) %>%
arrange(desc(tf_idf))
glimpse(review_words)
#Term frequencies across essays
tidy_review <- select(reviews,"comments")
n <- nrow(tidy_review)
tidy_review <- tibble(review=1:n, text=reviews$comments)
# Tokenize, remove stop words and NAs
data("stop_words")
#  Add some custom stop words
myterms <- c("airbnb","very", "singapore","stay")
mylex <- rep("custom", length(myterms))
myStopwords <- data.frame(word=myterms, lexicon=mylex)
myStopwords <- rbind(stop_words, myStopwords)
#Term frequencies
review_words <- tidy_review %>%
unnest_tokens(word, text) %>%
anti_join(myStopwords) %>%
count(review, word, sort=TRUE) %>%
ungroup()
total_words <- review_words %>%
group_by(review) %>%
summarise(total = sum(n))
review_words <- left_join(review_words, total_words)
#  now tf_idf
review_words <- review_words %>%
bind_tf_idf(word, review, n) %>%
arrange(desc(tf_idf))
glimpse(review_words)
